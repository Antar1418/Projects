{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9adc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark= SparkSession. \\\n",
    "builder. \\\n",
    "appName(\"DD_10\"). \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2851b0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:46497\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DD_10</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8dc4716240>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a1941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logschema = \"loglevel string, logtime timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636358be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(logschema) \\\n",
    ".load(\"/public/trendytech/datasets/logdata1m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ff9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|    INFO|2015-08-08 20:49:22|\n",
      "|    WARN|2015-01-14 20:05:00|\n",
      "|    INFO|2017-06-14 00:08:35|\n",
      "|    INFO|2016-01-18 11:50:14|\n",
      "|   DEBUG|2017-07-01 12:55:02|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf5cb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loglevel: string (nullable = true)\n",
      " |-- logtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb41447",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.createOrReplaceTempView(\"serverlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008da5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|    INFO|2015-08-08 20:49:22|\n",
      "|    WARN|2015-01-14 20:05:00|\n",
      "|    INFO|2017-06-14 00:08:35|\n",
      "|    INFO|2016-01-18 11:50:14|\n",
      "|   DEBUG|2017-07-01 12:55:02|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from serverlog\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2af47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d8f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|loglevel|   01|   02|   03|   04|   05|   06|   07|   08|   09|   10|   11|   12|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    INFO|29119|28983|29095|29302|28900|29143|29300|28993|29038|29018|23301|28874|\n",
      "|   ERROR| 4054| 4013| 4122| 4107| 4086| 4059| 3976| 3987| 4161| 4040| 3389| 4106|\n",
      "|    WARN| 8217| 8266| 8165| 8277| 8403| 8191| 8222| 8381| 8352| 8226| 6616| 8328|\n",
      "|   DEBUG|41961|41734|41652|41869|41785|41774|42085|42147|41433|41936|33366|41749|\n",
      "|   FATAL|   94|   72|   70|   83|   60|   78|   98|   80|   81|   92|16797|   94|\n",
      "+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select loglevel, date_format(logtime,'MM') as month\n",
    "from serverlog\"\"\").groupBy('loglevel').pivot('month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819a3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+--------+--------+-------+-----+-----+-----+-----+--------+-------+---------+\n",
      "|loglevel|April|August|December|February|January| July| June|March|  May|November|October|September|\n",
      "+--------+-----+------+--------+--------+-------+-----+-----+-----+-----+--------+-------+---------+\n",
      "|    INFO|29302| 28993|   28874|   28983|  29119|29300|29143|29095|28900|   23301|  29018|    29038|\n",
      "|   ERROR| 4107|  3987|    4106|    4013|   4054| 3976| 4059| 4122| 4086|    3389|   4040|     4161|\n",
      "|    WARN| 8277|  8381|    8328|    8266|   8217| 8222| 8191| 8165| 8403|    6616|   8226|     8352|\n",
      "|   FATAL|   83|    80|      94|      72|     94|   98|   78|   70|   60|   16797|     92|       81|\n",
      "|   DEBUG|41869| 42147|   41749|   41734|  41961|42085|41774|41652|41785|   33366|  41936|    41433|\n",
      "+--------+-----+------+--------+--------+-------+-----+-----+-----+-----+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select loglevel, date_format(logtime,'MMMM') as month\n",
    "from serverlog\"\"\").groupBy('loglevel').pivot('month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b778a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_list = [\n",
    "\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "\"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5041f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|loglevel|January|February|March|April|  May| June| July|August|September|October|November|December|\n",
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "|    INFO|  29119|   28983|29095|29302|28900|29143|29300| 28993|    29038|  29018|   23301|   28874|\n",
      "|   ERROR|   4054|    4013| 4122| 4107| 4086| 4059| 3976|  3987|     4161|   4040|    3389|    4106|\n",
      "|    WARN|   8217|    8266| 8165| 8277| 8403| 8191| 8222|  8381|     8352|   8226|    6616|    8328|\n",
      "|   FATAL|     94|      72|   70|   83|   60|   78|   98|    80|       81|     92|   16797|      94|\n",
      "|   DEBUG|  41961|   41734|41652|41869|41785|41774|42085| 42147|    41433|  41936|   33366|   41749|\n",
      "+--------+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select loglevel, date_format(logtime,'MMMM') as month\n",
    "from serverlog\"\"\").groupBy('loglevel').pivot('month',months_list).count().fillna(0).show()\n",
    "#Fill missing months with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367cddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c475a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
